{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML for Finance\n",
    "## Fall 2020\n",
    "## Lesson 7\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TECHNICAL CELL\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Transactions Data (https://www.dropbox.com/s/3oyrho6agd7clpm/transactions.csv?dl=1)\n",
    "transactions = pd.read_csv('transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day number\n",
    "transactions['day'] = transactions.tr_datetime.apply(lambda x: int(x.split(' ')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions per day\n",
    "gr = transactions.groupby('day')['amount'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Time Series\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.array(gr.index), np.array(gr))\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum\n",
    "np.argmin(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlarge a bit\n",
    "part_transaction = transactions.loc[np.logical_and(transactions.day >= 10, transactions.day <= 63)]\n",
    "\n",
    "gr = part_transaction.groupby('day')['amount'].count()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.array(gr.index), np.array(gr), 'o-')\n",
    "plt.plot([31, 31], [10000, 15000])\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Except working days\n",
    "part_transaction = transactions.loc[np.logical_and(transactions.day >= 31+28+31+30, transactions.day <= 31+28+31+30+15)]\n",
    "\n",
    "gr = part_transaction.groupby('day')['amount'].count()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.array(gr.index), np.array(gr), 'o-')\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housing Data (https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "# The file is also place in MS Teams (Files)\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = data.drop(columns=[\"Id\"])\n",
    "\n",
    "y = data[\"SalePrice\"]\n",
    "X = data.drop(columns=[\"SalePrice\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(36)\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.cos(1.5 * np.pi * x)\n",
    "\n",
    "x_objects = np.random.uniform(0, 1, size=30)\n",
    "y_objects = np.cos(1.5 * np.pi * x_objects) + np.random.normal(scale=0.1, size=x_objects.shape)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "fig, axs = plt.subplots(figsize=(16, 4), ncols=3)\n",
    "for i, degree in enumerate([1, 4, 20]):\n",
    "    X_objects = PolynomialFeatures(degree).fit_transform(x_objects[:, None])\n",
    "    X = PolynomialFeatures(degree).fit_transform(x[:, None])\n",
    "    regr = LinearRegression().fit(X_objects, y_objects)\n",
    "    y_pred = regr.predict(X)\n",
    "    axs[i].plot(x, y, label=\"Real function\")\n",
    "    axs[i].scatter(x_objects, y_objects, label=\"Data\")\n",
    "    axs[i].plot(x, y_pred, label=\"Prediction\")\n",
    "    if i == 0:\n",
    "        axs[i].legend()\n",
    "    axs[i].set_title(\"Degree = %d\" % degree)\n",
    "    axs[i].set_xlabel(\"$x$\")\n",
    "    axs[i].set_ylabel(\"$f(x)$\")\n",
    "    axs[i].set_ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "fig, axs = plt.subplots(figsize=(16, 5), ncols=3)\n",
    "for i, feature in enumerate([\"GrLivArea\", \"GarageArea\", \"TotalBsmtSF\"]):\n",
    "    axs[i].scatter(X_train[feature], y_train, alpha=0.2)\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel(\"SalePrice\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations\n",
    "numeric_data = X_train.select_dtypes([np.number])\n",
    "numeric_data_mean = numeric_data.mean()\n",
    "numeric_features = numeric_data.columns\n",
    "\n",
    "X_train = X_train.fillna(numeric_data_mean)\n",
    "X_test = X_test.fillna(numeric_data_mean)\n",
    "\n",
    "correlations = {\n",
    "    feature: np.corrcoef(X_train[feature], y_train)[0][1]\n",
    "    for feature in numeric_features\n",
    "}\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "features_order = [x[0] for x in sorted_correlations]\n",
    "correlations = [x[1] for x in sorted_correlations]\n",
    "\n",
    "plot = sns.barplot(y=features_order, x=correlations)\n",
    "plot.figure.set_size_inches(15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Square Error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = Ridge()\n",
    "model.fit(X_train[numeric_features], y_train)\n",
    "y_pred = model.predict(X_test[numeric_features])\n",
    "y_train_pred = model.predict(X_train[numeric_features])\n",
    "\n",
    "print(\"Test MSE = %.4f\" % mean_squared_error(y_test, y_pred))\n",
    "print(\"Train MSE = %.4f\" % mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More on linear models\n",
    "df = pd.read_csv('data/dataset.csv', sep = '\\t')\n",
    "df.index = range(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vs. Test\n",
    "df_train, df_test = train_test_split(df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Scatter\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "df_train.plot(x='CRIM', y='MEDV', kind='scatter', s=120);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(x='RM', y='MEDV', kind='scatter', s=120);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS\n",
    "\n",
    "class SimpleLinearRegression():\n",
    "    def __init__(self, fit_intercept = True):\n",
    "        self.coef_ = []\n",
    "        self.intercept_ = 0.0\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._solve(np.copy(X), np.copy(y))\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        #multiply X on self.coef and add intercept\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "\n",
    "    \n",
    "    def _solve(self, X, y):\n",
    "        #if we find intercept, sub mean from X and y \n",
    "        if self.fit_intercept:\n",
    "            #sub mean\n",
    "            X_offset = np.mean(X, axis = 0)\n",
    "            X -= X_offset\n",
    "        \n",
    "            y_offset = np.mean(y)\n",
    "            y -= y_offset\n",
    "\n",
    "        #analytical solution, A @ B is multiply A on B like matrix\n",
    "        self.coef_ = np.matmul(np.linalg.inv(X.T @ X), X.T).dot(y)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            self._set_intercept(X_offset, y_offset)\n",
    "\n",
    "\n",
    "    def _set_intercept(self, X_offset, y_offset):\n",
    "        self.intercept_ = y_offset - X_offset.dot(self.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.RM.values.reshape(-1, 1)\n",
    "y_train = df_train.MEDV.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "model = SimpleLinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model:\\nprice = %.2f + (%.2f)*rooms' % (model.intercept_, model.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('intercept = {}'.format(model.intercept_), 'coef = {}'.format(model.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Model\n",
    "df_train.plot(x = 'RM', y = 'MEDV', kind = 'scatter', s=120)\n",
    "\n",
    "#predict values\n",
    "y_hat = model.predict(X_train)\n",
    "\n",
    "plt.plot(X_train, y_hat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns = ['MEDV']).values\n",
    "y_train = df_train.MEDV.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\frac{1}{N} \\sum_n |\\hat{y}_{n}-y_n|$ - mean absolute error\n",
    "* $\\frac{1}{N} \\sum_n (\\hat{y}_{n}-y_n)^2$ - mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test.MEDV.values\n",
    "y_hat = model.predict(df_test.drop(columns = ['MEDV']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE %.2f' % mean_absolute_error(y_test, y_hat))\n",
    "print('MSE %.2f' % mean_squared_error(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "#fit Linear model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test.MEDV.values\n",
    "y_hat = model.predict(df_test.drop(columns = ['MEDV']).values)\n",
    "print('MAE %.2f' % mean_absolute_error(y_test, y_hat))\n",
    "print('MSE %.2f' % mean_squared_error(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers tracking\n",
    "X_train = df_train.RM.values.reshape(-1, 1)\n",
    "y_train = df_train.MEDV.values\n",
    "n = y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add random outliers\n",
    "for i in range(10):\n",
    "    X_train = np.r_[X_train, [[np.random.rand()*20]]]\n",
    "    y_train = np.r_[y_train, np.random.randn()*10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot It\n",
    "plt.scatter(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model It\n",
    "model = SimpleLinearRegression(fit_intercept=True)\n",
    "model.fit(X_train[:n], y_train[:n])\n",
    "\n",
    "model_ouliers = SimpleLinearRegression(fit_intercept=True)\n",
    "model_ouliers.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done Finally\n",
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model.predict(x)\n",
    "y_hat_outliers = model_ouliers.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red', label='good model')\n",
    "ax.plot(x, y_hat_outliers, c='green', label='biased model')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.datasets import make_moons, load_iris # import function from the library\n",
    "N = 1000\n",
    "X, y = make_moons(n_samples=N, noise=0.2, random_state=11) # generate data sample\n",
    "\n",
    "# Create an figure with a custom size\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Plot all objects with y == 0 (class 0)\n",
    "plt.scatter(X[y == 0, 0],     # selects all objects with y == 0 and the 1st column of X\n",
    "            X[y == 0, 1],     # selects all objects with y == 0 and the 2nd column of X\n",
    "            color='r',        # points color\n",
    "            label='0')        # label for the plot legend\n",
    "\n",
    "\n",
    "# Plot all objects with y == 1 (class 1)\n",
    "plt.scatter(X[y == 1, 0],     # selects all objects with y == 1 and the 1st column of X\n",
    "            X[y == 1, 1],     # selects all objects with y == 1 and the 2nd column of X\n",
    "            color='b',        # points color\n",
    "            label='1')        # label for the plot legend\n",
    "\n",
    "plt.xlabel('X1') # set up X-axis label\n",
    "plt.ylabel('X2') # set up Y-axis label\n",
    "\n",
    "plt.legend(loc='best') # create the plot legend and set up it position\n",
    "plt.grid(b=1) # create grid on the plot\n",
    "\n",
    "plt.show() # display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split function to split the sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.5,    # 20% for test, 80% for train\n",
    "                                                    random_state=123) # shuffle objects before split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X, y shapes: \", X.shape, y.shape)\n",
    "print(\"X_train, y_train shapes: \", X_train.shape, y_train.shape)\n",
    "print(\"X_test, y_test shapes: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier(object):\n",
    "    \n",
    "    def __init__(self, k_neighbors=1):\n",
    "        \"\"\"\n",
    "        This is a constructor of the class. \n",
    "        Here you can define parameters (k_neighbors) of the class and \n",
    "        attributes, that are visible within all methods of the class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        k_neighbors : int\n",
    "            Number of neighbors used for classification.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make this parameter visible in all methods of the class\n",
    "        self.k_neighbors = k_neighbors\n",
    "        \n",
    "        # Lets define variables for data, that will be used during the classifier fit and predict\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "                \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        This method trains the KNN classifier. \n",
    "        Actualy, the KNN classifier has no training procedure.\n",
    "        It just remembers data (X, y) that will be used for predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the object labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Just save X and y. There is no training procedure for KNN classifier\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    \n",
    "    def calculate_distances(self, X, one_x):\n",
    "        \"\"\"\n",
    "        This method calculates distances between one object and all other objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        one_x : numpy.array, shape = (n_features)\n",
    "        \"\"\"\n",
    "        \n",
    "        dists = np.sqrt( np.sum( (X - one_x)**2, axis=1 ) )\n",
    "        return dists\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs labels prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_predicted : numpy.array, shape = (n_objects)\n",
    "            1D array with predicted labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create an empty list for predicted labels\n",
    "        y_predicted = []\n",
    "        \n",
    "        # For each object in X make prediction\n",
    "        for one_x in X:\n",
    "            \n",
    "            # one_x = [0.2, 0.57] (example)\n",
    "            \n",
    "            # Calculate distances between an object and all objects from train smaple\n",
    "            distances = self.calculate_distances(self.X_train, one_x)\n",
    "            # distances = [0.25, 0.10, 0.32, 0.05] (example)\n",
    "            \n",
    "            # Sort the distances and get indeces of the sorted order\n",
    "            sorted_indeces = distances.argsort()\n",
    "            # sorted_indeces = [3, 1, 0, 2] (example)\n",
    "            \n",
    "            # Get k_neighbors from train sample with the smallest distances\n",
    "            k_neighbors_indeces = sorted_indeces[:self.k_neighbors] # take the first k_neighbors elements\n",
    "            # k_neighbors_indeces = [3, 1, 0], for k_neighbors=2 (example)\n",
    "            \n",
    "            # Get labels of these k_neighbors\n",
    "            k_neighbors_labels = self.y_train[k_neighbors_indeces]\n",
    "            # k_neighbors_labels = [0, 1, 0] (example)\n",
    "            \n",
    "            # Get list of unique labels and counts of each label\n",
    "            unique_labels, label_counts = np.unique(k_neighbors_labels, return_counts=True)\n",
    "            # unique_labels = [0, 1] (example)\n",
    "            # label_counts  = [2, 1] (example)\n",
    "            \n",
    "            # Get label with the maximum count\n",
    "            label_max_count = unique_labels[label_counts == label_counts.max()][0]\n",
    "            # label_max_count = 0 (example)\n",
    "            \n",
    "            # Save the predicted label\n",
    "            y_predicted.append(label_max_count)\n",
    "            \n",
    "        return np.array(y_predicted) # return numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNN classifier object\n",
    "knn = KNNClassifier(k_neighbors=2)\n",
    "\n",
    "# Train the classifier (remember, that KNN has no training procedure, but it is a tradition :)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Use the classifier to predict labels\n",
    "y_test_predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out the first 20 predicted labels\n",
    "y_test_predict[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out the first 20 true test labels\n",
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an figure with a custom size\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Plot all objects with y == 0 (class 0)\n",
    "plt.scatter(X_test[y_test == 0, 0],     # selects all objects with y == 0 and the 1st column of X\n",
    "            X_test[y_test == 0, 1],     # selects all objects with y == 0 and the 2nd column of X\n",
    "            color='r',                  # points color\n",
    "            label='0')                  # label for the plot legend\n",
    "\n",
    "\n",
    "# Plot all objects with y == 1 (class 1)\n",
    "plt.scatter(X_test[y_test == 1, 0],     # selects all objects with y == 1 and the 1st column of X\n",
    "            X_test[y_test == 1, 1],     # selects all objects with y == 1 and the 2nd column of X\n",
    "            color='b',                  # points color\n",
    "            label='1')                  # label for the plot legend\n",
    "\n",
    "\n",
    "### THIS IS JUST MAGIC :)\n",
    "\n",
    "# plot decision boundary\n",
    "h=0.1\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=.6, levels=1)\n",
    "\n",
    "### THE END OF THE MAGIC\n",
    "\n",
    "\n",
    "plt.xlabel('X1') # set up X-axis label\n",
    "plt.ylabel('X2') # set up Y-axis label\n",
    "\n",
    "plt.legend(loc='best') # create the plot legend and set up it position\n",
    "plt.grid(b=1) # create grid on the plot\n",
    "\n",
    "plt.show() # display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the quality using **accuracy score**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy }(y\\_true, y\\_predict) = \\frac{1}{N} \\sum_{i=1}^{N} I(y\\_predict_{i} == y\\_true_{i})\n",
    "$$\n",
    "\n",
    "For the accuracy calculation we use function **accuracy_score** from the scukit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy_score function\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy for the test sample\n",
    "accuracy_test = accuracy_score(y_test, y_test_predict)\n",
    "\n",
    "print(\"Test accuracy of KNN classifier: \", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic\n",
    "#generate two linear separated samples\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(20, 2) + [2, 2],\n",
    "          np.random.randn(20, 2) + [-2, -2]]\n",
    "y = [-1] * 20 + [1] * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(X[:, 0],\n",
    "           X[:, 1],\n",
    "           c=y,\n",
    "           cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLogisticRegression:\n",
    "    def __init__(self, C = 1.0, fit_intercept = True, penalty = 'l2', max_iter = 5000):\n",
    "        self.C = C\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.penalty = penalty\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):        \n",
    "        #zero initialization\n",
    "        self.coef_ = np.zeros(X.shape[1])\n",
    "        self.intercept_ = 0.0\n",
    "        \n",
    "        #run grad descent\n",
    "        self.qual_ = self.grad_descent(X, y, 0.05)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return np.array([1 / (1 + np.exp(X.dot(self.coef_) + self.intercept_)),\\\n",
    "                         1 / (1 + np.exp(-X.dot(self.coef_) - self.intercept_))])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict = (self.predict_proba(X)[1, :] > 0.5).astype(int)\n",
    "        \n",
    "        #transform 0 to -1\n",
    "        predict[predict == 0] = -1\n",
    "        \n",
    "        return predict\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "\n",
    "    #labels of classes {1, -1}\n",
    "    def loss(self, X, y):\n",
    "        loss = np.mean(np.log(1 + np.exp((-X.dot(self.coef_) + self.intercept_) * y)))\n",
    "        if self.penalty == 'l2':\n",
    "            reg = np.sum(self.coef_ ** 2)\n",
    "            if self.fit_intercept:\n",
    "                reg += self.intercept_ ** 2\n",
    "        elif self.penalty == 'l1':\n",
    "            reg = np.sum(np.abs(self.coef_))\n",
    "            if self.fit_intercept:\n",
    "                reg += np.abs(self.intercept_)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return loss + reg / (self.C * X.shape[0])\n",
    "    \n",
    "    def grad_descent(self, X, y, eta ):\n",
    "        qual = np.zeros(self.max_iter)\n",
    "        \n",
    "        for iterations in range(self.max_iter):\n",
    "            #calculate gradient\n",
    "            decision_function = -(X.dot(self.coef_) + self.intercept_)\n",
    "            grad_coef = -np.exp(decision_function * y) / (1 + np.exp(decision_function * y))\n",
    "            grad_coef *= y\n",
    "            \n",
    "            if isinstance(X, np.ndarray):\n",
    "                mean_grad = np.mean(X.T * grad_coef, axis = 1)\n",
    "            elif isinstance(X, scipy.sparse.csr.csr_matrix):\n",
    "                mean_grad = np.array(np.mean(X.T.multiply(grad_coef), axis = 1)[:, 0]).reshape(-1)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            w = np.copy(self.coef_)\n",
    "            self.coef_ -= eta * mean_grad\n",
    "            \n",
    "            if self.fit_intercept:\n",
    "                w_0 = self.intercept_\n",
    "                self.intercept_ -= eta * np.mean(grad_coef)\n",
    "    \n",
    "            #add penalty\n",
    "            if self.penalty == 'l2':\n",
    "                self.coef_ -= 2 * eta * w / (self.C * X.shape[0])\n",
    "                if self.fit_intercept:\n",
    "                    self.intercept_ -= 2 * eta * w_0 / (self.C * X.shape[0])\n",
    "            elif self.penalty == 'l1':\n",
    "                self.coef_ -= eta * np.sign(w) / (self.C * X.shape[0])\n",
    "                if self.fit_intercept:\n",
    "                    self.intercept_ -= eta * np.sign(w_0) / (self.C * X.shape[0])\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            qual[iterations] = self.loss(X, y)\n",
    "\n",
    "        return qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLogisticRegression(C=1.0, \n",
    "                           fit_intercept=True, \n",
    "                           penalty='l2')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w_0 = %f' % model.intercept_)\n",
    "print('w_1, w_2 = ', model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_proba = model.predict_proba(X)\n",
    "y_hat_proba[:20, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_func = model.decision_function(X)\n",
    "dec_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "x0, x1 = np.meshgrid(np.arange(-3, 3, 0.1),\n",
    "                       np.arange(-3, 3, 0.1))\n",
    "xx0, xx1 = x0.ravel(), x1.ravel()\n",
    "\n",
    "X_grid = np.c_[xx0, xx1, ]\n",
    "\n",
    "y_hat = model.decision_function(X_grid)\n",
    "y_hat = y_hat.reshape(x0.shape)\n",
    "\n",
    "plt.contour(x0, x1, y_hat, levels=[0])\n",
    "\n",
    "\n",
    "ax.scatter(X[:, 0],\n",
    "           X[:, 1],\n",
    "           c=y,\n",
    "           cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality vs. Loss\n",
    "qual = model.qual_\n",
    "x = np.arange(0, len(qual))\n",
    "plt.title('quality')\n",
    "plt.plot(x, qual)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('log-loss with l2');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
